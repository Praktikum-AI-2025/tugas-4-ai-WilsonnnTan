{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m7EKCPIxp3hI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50f821c-84fc-49f6-a792-0ce4f112eb47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from keras_preprocessing) (2.0.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from keras_preprocessing) (1.17.0)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "G2sgrSUHp73U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solution_05():\n",
        "    data_url_1 = 'https://github.com/dicodingacademy/assets/releases/download/release-horse-or-human/horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_1, 'horse-or-human.zip')\n",
        "    local_file = 'horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/horse-or-human')\n",
        "\n",
        "    data_url_2 = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/validation-horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_2, 'validation-horse-or-human.zip')\n",
        "    local_file = 'validation-horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/validation-horse-or-human')\n",
        "    zip_ref.close()\n",
        "\n",
        "    # train data\n",
        "    TRAINING_DIR = 'data/horse-or-human'\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1/255,\n",
        "        # rotation_range=40,\n",
        "        # horizontal_flip=True,\n",
        "        # shear_range=0.2,\n",
        "        # zoom_range=0.2,\n",
        "        # fill_mode='nearest'\n",
        "    )\n",
        "    # menurut saya, train_datagen tidak perlu di augmentasi karena dataset masih kecil\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR, target_size=(150, 150), class_mode='binary')\n",
        "\n",
        "    # validation data\n",
        "    VALIDATION_DIR = 'data/validation-horse-or-human'\n",
        "    validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR, target_size=(150, 150), class_mode='binary')\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    model=tf.keras.models.Sequential([\n",
        "        # YOUR CODE HERE, end with a Neuron Dense, activated by sigmoid\n",
        "        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid') #DO NOT CHANGE THIS LINE!\n",
        "        ])\n",
        "\n",
        "    # merupkan bagian yang memilih algo untuk menghitung loss dan membenarkan weight dan bias\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # menyimpan model dengan val accuracy tertinggi\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(filepath=\"best_val_acc_model.h5\", monitor='val_accuracy', mode='max', save_best_only=True, verbose=1),\n",
        "    ]\n",
        "\n",
        "    model.fit(train_generator, epochs=20, validation_data=validation_generator, callbacks=callbacks)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "MFsXOdIRp-RE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_05()\n",
        "    model.save(\"model_05.h5\")"
      ],
      "metadata": {
        "id": "22Mir4NGp_6x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1d1125-964f-4f2b-84dd-b39aee1c1a5c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.6796 - loss: 0.6049\n",
            "Epoch 1: val_accuracy improved from -inf to 0.76562, saving model to best_val_acc_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 230ms/step - accuracy: 0.6832 - loss: 0.6001 - val_accuracy: 0.7656 - val_loss: 1.9001\n",
            "Epoch 2/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.9530 - loss: 0.1503\n",
            "Epoch 2: val_accuracy did not improve from 0.76562\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 178ms/step - accuracy: 0.9523 - loss: 0.1523 - val_accuracy: 0.6680 - val_loss: 1.6620\n",
            "Epoch 3/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8943 - loss: 0.2488\n",
            "Epoch 3: val_accuracy improved from 0.76562 to 0.85547, saving model to best_val_acc_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 197ms/step - accuracy: 0.8948 - loss: 0.2475 - val_accuracy: 0.8555 - val_loss: 0.8882\n",
            "Epoch 4/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9641 - loss: 0.0932\n",
            "Epoch 4: val_accuracy improved from 0.85547 to 0.86719, saving model to best_val_acc_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 179ms/step - accuracy: 0.9644 - loss: 0.0927 - val_accuracy: 0.8672 - val_loss: 0.9297\n",
            "Epoch 5/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.9938 - loss: 0.0229\n",
            "Epoch 5: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 193ms/step - accuracy: 0.9936 - loss: 0.0229 - val_accuracy: 0.8359 - val_loss: 1.7282\n",
            "Epoch 6/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9947 - loss: 0.0184\n",
            "Epoch 6: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 172ms/step - accuracy: 0.9946 - loss: 0.0184 - val_accuracy: 0.8398 - val_loss: 1.9144\n",
            "Epoch 7/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 0.0053\n",
            "Epoch 7: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 193ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.8594 - val_loss: 1.9394\n",
            "Epoch 8/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9990 - loss: 0.0047\n",
            "Epoch 8: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 175ms/step - accuracy: 0.9990 - loss: 0.0047 - val_accuracy: 0.8320 - val_loss: 2.4808\n",
            "Epoch 9/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 0.0021\n",
            "Epoch 9: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.8594 - val_loss: 1.8959\n",
            "Epoch 10/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 3.7891e-04\n",
            "Epoch 10: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 181ms/step - accuracy: 1.0000 - loss: 3.7826e-04 - val_accuracy: 0.8594 - val_loss: 2.3451\n",
            "Epoch 11/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 1.6408e-04\n",
            "Epoch 11: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 192ms/step - accuracy: 1.0000 - loss: 1.6532e-04 - val_accuracy: 0.8477 - val_loss: 2.6363\n",
            "Epoch 12/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 1.1316e-04\n",
            "Epoch 12: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 1.1357e-04 - val_accuracy: 0.8555 - val_loss: 2.5924\n",
            "Epoch 13/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 1.0830e-04\n",
            "Epoch 13: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 1.0000 - loss: 1.0814e-04 - val_accuracy: 0.8516 - val_loss: 2.6930\n",
            "Epoch 14/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 7.5672e-05\n",
            "Epoch 14: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 175ms/step - accuracy: 1.0000 - loss: 7.5926e-05 - val_accuracy: 0.8516 - val_loss: 2.7559\n",
            "Epoch 15/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 8.5057e-05\n",
            "Epoch 15: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 192ms/step - accuracy: 1.0000 - loss: 8.4617e-05 - val_accuracy: 0.8555 - val_loss: 2.7873\n",
            "Epoch 16/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 4.8770e-05\n",
            "Epoch 16: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 4.9067e-05 - val_accuracy: 0.8516 - val_loss: 2.8368\n",
            "Epoch 17/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 5.4258e-05\n",
            "Epoch 17: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 193ms/step - accuracy: 1.0000 - loss: 5.4176e-05 - val_accuracy: 0.8516 - val_loss: 2.9232\n",
            "Epoch 18/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 6.4331e-05\n",
            "Epoch 18: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 175ms/step - accuracy: 1.0000 - loss: 6.3800e-05 - val_accuracy: 0.8516 - val_loss: 2.8893\n",
            "Epoch 19/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 3.9002e-05\n",
            "Epoch 19: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 1.0000 - loss: 3.9031e-05 - val_accuracy: 0.8516 - val_loss: 2.9579\n",
            "Epoch 20/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 4.3066e-05\n",
            "Epoch 20: val_accuracy did not improve from 0.86719\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 4.2847e-05 - val_accuracy: 0.8477 - val_loss: 3.0687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "CnsJsKVxpqKi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_akhir = load_model(\"model_05.h5\")\n",
        "model_best_val_accuracy = load_model(\"best_val_acc_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GxrmxpMps4w",
        "outputId": "48d74b55-331f-4e27-a0aa-4b72ca42da69"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation data\n",
        "VALIDATION_DIR = 'data/validation-horse-or-human'\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR, target_size=(150, 150), class_mode='binary', batch_size=32)\n",
        "\n",
        "# Evaluate model_akhir\n",
        "loss_akhir, acc_akhir = model_akhir.evaluate(validation_generator)\n",
        "print(f\"Validation Accuracy (model_akhir): {acc_akhir * 100:.2f}%\")\n",
        "\n",
        "# Evaluate model_best_val_accuracy\n",
        "loss_best, acc_best = model_best_val_accuracy.evaluate(validation_generator)\n",
        "print(f\"Validation Accuracy (best_val_acc_model): {acc_best * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJOTirCapy9m",
        "outputId": "678367ed-1102-428e-eb4a-e6d3d0e09519"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 256 images belonging to 2 classes.\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8539 - loss: 2.9086\n",
            "Validation Accuracy (model_akhir): 84.77%\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8567 - loss: 1.0688\n",
            "Validation Accuracy (best_val_acc_model): 86.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DIR = 'data/horse-or-human'\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR, target_size=(150, 150), class_mode='binary', batch_size=128)\n",
        "\n",
        "# Evaluate model_akhir\n",
        "loss_akhir, acc_akhir = model_akhir.evaluate(train_generator)\n",
        "print(f\"Validation Accuracy (model_akhir): {acc_akhir * 100:.2f}%\")\n",
        "\n",
        "# Evaluate model_best_val_accuracy\n",
        "loss_best, acc_best = model_best_val_accuracy.evaluate(train_generator)\n",
        "print(f\"Validation Accuracy (best_val_acc_model): {acc_best * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYjSOFI5p08o",
        "outputId": "eed644d2-873f-4783-e322-c0871ba89907"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 657ms/step - accuracy: 1.0000 - loss: 3.0690e-05\n",
            "Validation Accuracy (model_akhir): 100.00%\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 629ms/step - accuracy: 0.9864 - loss: 0.0368\n",
            "Validation Accuracy (best_val_acc_model): 98.73%\n"
          ]
        }
      ]
    }
  ]
}